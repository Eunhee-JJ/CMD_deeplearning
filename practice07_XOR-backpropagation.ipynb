{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR 문제를 사용해 오차 역전파를 코드로 나타내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NeuralNetwork' object has no attribute 'update'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-17bebf21a795>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;31m#학습 실행\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m     \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m#결괏값 출력\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-17bebf21a795>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, patterns)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mtargets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m                 \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackPropagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NeuralNetwork' object has no attribute 'update'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "#1. 환경 변수 설정하기\n",
    "\n",
    "#입력 값 및 타깃 값\n",
    "data=[\n",
    "    [[0,0],[0]],\n",
    "    [[0,1],[0]],\n",
    "    [[1,0],[1]],\n",
    "    [[1,1],[0]],\n",
    "]\n",
    "\n",
    "#실행 횟수(iterations), 학습률(lr), 모멘텀 계수(mo) 설정\n",
    "iterations = 5000\n",
    "lr=0.1\n",
    "mo=0.9\n",
    "\n",
    "#활성화 함수 - 1. 시그모이드\n",
    "#미분할 때와 아닐 때의 각각의 값\n",
    "def sigmoid(x, derivative = False): #이게 무슨 문법이람?\n",
    "    if(derivative == True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#활성화 함수 - 2. tanh\n",
    "#tanh 함수의 미분은 1-(활성화 함수 출력의 제곱)\n",
    "def tanh(x, derivative = False):\n",
    "    if(derivative == True):\n",
    "        return 1-x**2\n",
    "    return np.tanh(x)\n",
    "\n",
    "#가중치 배열을 만드는 함수\n",
    "def makeMatrix(i, j, fill = 0.0):\n",
    "    mat=[]\n",
    "    for i in range(i):\n",
    "        mat.append([fill] *j)\n",
    "    return mat\n",
    "\n",
    "\n",
    "#2. 신경망의 실행\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    #초깃값 지정\n",
    "    def __init__(self, num_x, num_yh, num_yo, bias=1):\n",
    "        \n",
    "        #입력 값(num_x), 은닉층의 초깃값(num_yh), 출력층의 초깃값(num_yo), 바이어스\n",
    "        self.num_x = num_x + bias #바이어스는 1로 설정 -> 왜 여기서 더하지?????\n",
    "        self.num_yh=num_yh\n",
    "        self.num_yo=num_yo\n",
    "        \n",
    "        #활성화 함수 (출력값) 초깃값\n",
    "        self.activation_input=[1.0] * self.num_x #이게 무슨 문법이람?2 => 리스트 연산-반복!\n",
    "        self.activation_hidden=[1.0] * self.num_yh\n",
    "        self.activation_out=[1.0]*self.num_yo\n",
    "        \n",
    "        #가중치 입력 (출력값) 초깃값\n",
    "        self.weight_in=makeMatrix(self.num_x, self.num_yh)\n",
    "        for i in range(self.num_x):\n",
    "            for j in range(self.num_yh):\n",
    "                self.weight_in[i][j]=random.random()\n",
    "                \n",
    "        #가중치 출력 초깃값\n",
    "        self.weight_out = makeMatrix(self.num_yh, self.num_yo)\n",
    "        for j in range(self.num_yh):\n",
    "            for k in range(self.num_yo):\n",
    "                self.weight_out[j][k]=random.random()\n",
    "                \n",
    "        #모멘텀 SGD를 위한 이전 가중치 초깃값 -> ????\n",
    "        self.gradient_in=makeMatrix(self.num_x, self.num_yh)\n",
    "        self.gradient_out=makeMatrix(self.num_yh, self.num_yo)\n",
    "        \n",
    "        #업데이트 함수\n",
    "        def update(self, inputs):\n",
    "            \n",
    "            #입력층의 활성화 함수\n",
    "            for i in range(self.num_x-1):\n",
    "                self.activation_input[i]=inputs[i]\n",
    "                \n",
    "            #은닉층의 활성화 함수\n",
    "            for j in range(self.num_yh):\n",
    "                sum=0.0\n",
    "                for i in range(self.num_x):\n",
    "                    sum=sum+self.activation_input[i]*self.weight_in[i][j]\n",
    "                    \n",
    "                #활성화 함수 - 시그모이드와 tanh 중 선택\n",
    "                self.activation_hidden[j]=tanh(sum,False)\n",
    "                \n",
    "            #출력층의 활성화 함수\n",
    "            for k in range(self.num_yo):\n",
    "                sum=0.0\n",
    "                for j in range(self.num_yh):\n",
    "                    sum=sum_self.activation_hidden[j] * self. weight_out[j][k]\n",
    "                    \n",
    "                #시그모이드와 tanh 중에서 활성화 함수 선택\n",
    "                self.activation_out[k]=tanh(sum,False)\n",
    "                \n",
    "            return self.activation_out[:]\n",
    "        \n",
    "        \n",
    "    #역전파 실행\n",
    "    def backPropagate(self, targets):\n",
    "\n",
    "        #델타 출력 계산\n",
    "        output_deltas=[0.0]*self.sum_yo\n",
    "        for k in range(self.num_yo):\n",
    "            error=targets[k]-self.activation_out[k]\n",
    "\n",
    "            #시그모이드와 tanh 중에서 활성화 함수 선택, 미분 적용\n",
    "            output_deltas[k] = tanh(self.activation_out[k], True) * error\n",
    "\n",
    "        #은닉 노드의 오차 함수\n",
    "        hidden_deltas=[0.0] * self.num_yh\n",
    "\n",
    "        for j in range(self.num_yh):\n",
    "            error=0.0\n",
    "            for k in tange(self.num_you):\n",
    "                error = error + ouput_deltas[k]*self.weight_out[j][k]\n",
    "                #시그모이드와 tanh 중에서 활성화 함수 선택, 미분 적용\n",
    "\n",
    "            hideen_deltas[j]=tanh(self.activation_hidden[j], True)*error\n",
    "\n",
    "        #출력 가중치 업데이트\n",
    "        for j in range(self.num_yh):\n",
    "            for k in range(self.num_you):\n",
    "                gradient=ouput_deltas[k]*self.activation_hidden[j]\n",
    "                v=mo*self.gradeint_in[j][k]-lr*gradeint\n",
    "                self.weight_in[j][k]+=v\n",
    "                self.gradeint_out[j][k]=gradeint\n",
    "\n",
    "        #입력 가중치 업데이트\n",
    "        for i in range(self.num_x):\n",
    "            for j in range(self.num_yh):\n",
    "                gradient = hidden_deltas[j] * self.activation_input[i]\n",
    "                v=mo*self.gradeint_in[i][j] - lr*gradient\n",
    "                self.weight_in[i][j] += v\n",
    "                self.gradeint_in[i][j] = gradient\n",
    "\n",
    "        #오차 계산(최소 제곱법)\n",
    "        error = 0.0\n",
    "        for k in range(len(targets)):\n",
    "            error = error + 0.5 * (targets[k] - self.activation_out[k])**2\n",
    "        return error\n",
    "\n",
    "\n",
    "    #학습 실행\n",
    "    def train(self, patterns):\n",
    "        for i in range(iterations):\n",
    "            error = 0.0\n",
    "            for p in patterns :\n",
    "                inputs=p[0]\n",
    "                targets=p[1]\n",
    "                self.update(inputs)\n",
    "                error=error+self.backPropagate(targets)\n",
    "            if i%500==0:\n",
    "                print('error: %-.5f' % error)\n",
    "\n",
    "    #결과값 출력\n",
    "    def result(self, patterns):\n",
    "        for p in patterns :\n",
    "            print('Input: %s, Predict: %s' %(p[0], self.update(p[0])))\n",
    "        \n",
    "if __name__ =='__main__':\n",
    "    #두 개의 입력 값, 두 개의 층, 하나의 출력 값을 갖도록 설정\n",
    "    n=NeuralNetwork(2,2,1)\n",
    "    \n",
    "    #학습 실행\n",
    "    n.train(data)\n",
    "    \n",
    "    #결괏값 출력\n",
    "    n.result(data)\n",
    "\n",
    "# Reference: http://arctrix.com/nas/python/bpnn.py (Neil Schemenauer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
